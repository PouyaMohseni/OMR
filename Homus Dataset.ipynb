{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad20992",
   "metadata": {},
   "source": [
    "# HOMUS Dataset -- Construct and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a69e4",
   "metadata": {},
   "source": [
    "Download the homus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b04aa2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "HOMUS_V2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momrdatasettools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Downloader, OmrDataset\n\u001b[0;32m      3\u001b[0m downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[1;32m----> 4\u001b[0m downloader\u001b[38;5;241m.\u001b[39mdownload_and_extract_dataset(\u001b[43mOmrDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHOMUS_V2\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\enum.py:789\u001b[0m, in \u001b[0;36mEnumType.__getattr__\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_map_[name]\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: HOMUS_V2"
     ]
    }
   ],
   "source": [
    "from omrdatasettools import Downloader, OmrDataset\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_and_extract_dataset(OmrDataset.HOMUS_V2, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66137fba",
   "metadata": {},
   "source": [
    "Generate pictures from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 images with 0 symbols in 1 different stroke thicknesses ([3])\n",
      "Randomly drawn on a fixed canvas of size 96x96 (Width x Height)\n",
      "In directory C:\\Users\\admin\\Desktop\\image processing\\project\\MohseniPouya-DIPproject\\homus_data_96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from omrdatasettools import HomusImageGenerator\n",
    "\n",
    "HomusImageGenerator.create_images(raw_data_directory=\"HOMUS\",\n",
    "                                  destination_directory=\"homus_data_96\",\n",
    "                                  stroke_thicknesses=[3],\n",
    "                                  canvas_width=96,\n",
    "                                  canvas_height=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Construct train test, and validation datasets by spliting the pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(original_path, new_path, train_ratio, test_ratio, validation_ratio):\n",
    "    # Get the list of class directories in the original dataset\n",
    "    class_directories = os.listdir(original_path)\n",
    "\n",
    "    # Create the new directory structure for train-test-validation\n",
    "    new_train_path = os.path.join(new_path, 'train')\n",
    "    new_test_path = os.path.join(new_path, 'test')\n",
    "    new_validation_path = os.path.join(new_path, 'validation')\n",
    "\n",
    "    os.makedirs(new_train_path, exist_ok=True)\n",
    "    os.makedirs(new_test_path, exist_ok=True)\n",
    "    os.makedirs(new_validation_path, exist_ok=True)\n",
    "\n",
    "    for class_dir in class_directories:\n",
    "        class_path = os.path.join(original_path, class_dir)\n",
    "        new_train_class_path = os.path.join(new_train_path, class_dir)\n",
    "        new_test_class_path = os.path.join(new_test_path, class_dir)\n",
    "        new_validation_class_path = os.path.join(new_validation_path, class_dir)\n",
    "\n",
    "        os.makedirs(new_train_class_path, exist_ok=True)\n",
    "        os.makedirs(new_test_class_path, exist_ok=True)\n",
    "        os.makedirs(new_validation_class_path, exist_ok=True)\n",
    "\n",
    "        class_files = os.listdir(class_path)\n",
    "        random.shuffle(class_files)\n",
    "\n",
    "        num_samples = len(class_files)\n",
    "        num_train_samples = int(num_samples * train_ratio)\n",
    "        num_test_samples = int(num_samples * test_ratio)\n",
    "        num_validation_samples = num_samples - num_train_samples - num_test_samples\n",
    "\n",
    "        # Split the data for this class into train, test, and validation sets\n",
    "        train_files = class_files[:num_train_samples]\n",
    "        test_files = class_files[num_train_samples:num_train_samples + num_test_samples]\n",
    "        validation_files = class_files[num_train_samples + num_test_samples:]\n",
    "\n",
    "        # Move files to the respective directories\n",
    "        for file in train_files:\n",
    "            src_path = os.path.join(class_path, file)\n",
    "            dest_path = os.path.join(new_train_class_path, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "        for file in test_files:\n",
    "            src_path = os.path.join(class_path, file)\n",
    "            dest_path = os.path.join(new_test_class_path, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "        for file in validation_files:\n",
    "            src_path = os.path.join(class_path, file)\n",
    "            dest_path = os.path.join(new_validation_class_path, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "\n",
    "original_dataset_path = \"homus_data_96\"\n",
    "new_dataset_path = \"homus_split\"\n",
    "train_ratio = 0.85\n",
    "test_ratio = 0.1\n",
    "validation_ratio = 0.05\n",
    "\n",
    "split_dataset(original_dataset_path, new_dataset_path, train_ratio, test_ratio, validation_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a110a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
